{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from pathlib import Path\n",
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnab Hati\\Documents\\Job_Preparation\\Generative_AI\\Projects\\Chatbot-using-Llama2\\research\n"
     ]
    }
   ],
   "source": [
    "path = Path.cwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arnab Hati\\Documents\\Job_Preparation\\Generative_AI\\Projects\\Chatbot-using-Llama2\n"
     ]
    }
   ],
   "source": [
    "parent_directory = str(path.parent)\n",
    "print(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(parent_directory+\"\\\\data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of my chunk: 7020\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheGALE\n",
      "ENCYCLOPEDIA\n",
      "ofMEDICINE\n",
      "SECOND EDITION\n",
      "JACQUELINE L. LONGE, EDITOR\n",
      "DEIRDRE S. BLANCHFIELD, ASSOCIATE EDITOR\n",
      "VOLUME\n",
      "A-B1\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the embedded text into vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc=Pinecone(api_key=\"cc6791ba-09d2-48c0-83b1-52804f61f8e3\")\n",
    "index = pc.Index(\"medical-chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty Pinecone index\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "PineconeConfigurationError",
     "evalue": "You haven't specified an Api-Key.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPineconeConfigurationError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m docsearch\u001b[38;5;241m=\u001b[39mPineconeVectorStore\u001b[38;5;241m.\u001b[39mfrom_texts([t\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text_chunks], embeddings, index_name\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:438\u001b[0m, in \u001b[0;36mPineconeVectorStore.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, batch_size, text_key, namespace, index_name, upsert_kwargs, pool_threads, embeddings_chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PineconeVectorStore:\n\u001b[0;32m    411\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct Pinecone wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 438\u001b[0m     pinecone_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_pinecone_index(index_name, pool_threads)\n\u001b[0;32m    439\u001b[0m     pinecone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(pinecone_index, embedding, text_key, namespace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    441\u001b[0m     pinecone\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    442\u001b[0m         texts,\n\u001b[0;32m    443\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(upsert_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    449\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:374\u001b[0m, in \u001b[0;36mPineconeVectorStore.get_pinecone_index\u001b[1;34m(cls, index_name, pool_threads, pinecone_api_key)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a Pinecone Index instance.\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    Pinecone Index instance.\"\"\"\u001b[39;00m\n\u001b[0;32m    373\u001b[0m _pinecone_api_key \u001b[38;5;241m=\u001b[39m pinecone_api_key \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPINECONE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 374\u001b[0m client \u001b[38;5;241m=\u001b[39m PineconeClient(\n\u001b[0;32m    375\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m_pinecone_api_key, pool_threads\u001b[38;5;241m=\u001b[39mpool_threads, source_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    376\u001b[0m )\n\u001b[0;32m    377\u001b[0m indexes \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mlist_indexes()\n\u001b[0;32m    378\u001b[0m index_names \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indexes\u001b[38;5;241m.\u001b[39mindex_list[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindexes\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\control\\pinecone.py:185\u001b[0m, in \u001b[0;36mPinecone.__init__\u001b[1;34m(self, api_key, host, proxy_url, proxy_headers, ssl_ca_certs, ssl_verify, config, additional_headers, pool_threads, index_api, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m PineconeConfig\u001b[38;5;241m.\u001b[39mbuild(\n\u001b[0;32m    186\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key, \n\u001b[0;32m    187\u001b[0m         host\u001b[38;5;241m=\u001b[39mhost,\n\u001b[0;32m    188\u001b[0m         additional_headers\u001b[38;5;241m=\u001b[39madditional_headers,\n\u001b[0;32m    189\u001b[0m         proxy_url\u001b[38;5;241m=\u001b[39mproxy_url,\n\u001b[0;32m    190\u001b[0m         proxy_headers\u001b[38;5;241m=\u001b[39mproxy_headers,\n\u001b[0;32m    191\u001b[0m         ssl_ca_certs\u001b[38;5;241m=\u001b[39mssl_ca_certs,\n\u001b[0;32m    192\u001b[0m         ssl_verify\u001b[38;5;241m=\u001b[39mssl_verify,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenapi_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    197\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing openapi_config is deprecated and will be removed in a future release. Please pass settings such as proxy_url, proxy_headers, ssl_ca_certs, and ssl_verify directly to the Pinecone constructor as keyword arguments. See the README at https://github.com/pinecone-io/pinecone-python-client for examples.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\config\\pinecone_config.py:24\u001b[0m, in \u001b[0;36mPineconeConfig.build\u001b[1;34m(api_key, host, additional_headers, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     22\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIgnoring PINECONE_ADDITIONAL_HEADERS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ConfigBuilder\u001b[38;5;241m.\u001b[39mbuild(api_key\u001b[38;5;241m=\u001b[39mapi_key, host\u001b[38;5;241m=\u001b[39mhost, additional_headers\u001b[38;5;241m=\u001b[39madditional_headers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\config\\config.py:54\u001b[0m, in \u001b[0;36mConfigBuilder.build\u001b[1;34m(api_key, host, proxy_url, proxy_headers, ssl_ca_certs, ssl_verify, additional_headers, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m source_tag \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(SOURCE_TAG, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeConfigurationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou haven\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt specified an Api-Key.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeConfigurationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou haven\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt specified a host.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mPineconeConfigurationError\u001b[0m: You haven't specified an Api-Key."
     ]
    }
   ],
   "source": [
    "docsearch=PineconeVectorStore.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty Pinecone index\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
